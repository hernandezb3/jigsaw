{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import kagglehub\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize LLM\n",
    "\n",
    "Start Ollama Docker container (see Resources/docker.sh) and call the model specified in the model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model paths\n",
    "GEMMA = \"google/gemma-3/transformers/gemma-3-1b-it\"\n",
    "LLAMA = \"\"\n",
    "\n",
    "# choose model\n",
    "model_id = GEMMA\n",
    "\n",
    "# import a model\n",
    "MODEL_PATH = kagglehub.model_download(model_id)\n",
    "\n",
    "# determine if gpu is available\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose pipeline criteria\n",
    "task = \"text-generation\"\n",
    "weights = {\"torch_dtype\": torch.bfloat16} # quantization = 16, 32, 64\n",
    "\n",
    "# initate pipeline\n",
    "pipeline = transformers.pipeline(task, model = MODEL_PATH, model_kwargs = weights) # initate pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data\n",
    "Data are from the Kaggle Competition, Jigsaw - Agile Community Rules Classification.\n",
    "\n",
    "Save the following files to the data folder:\n",
    "- train.csv\n",
    "- test.csv\n",
    "- sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Prompts\n",
    "Create a loop that first uses information from df to generate a prompt, supply it to the LLM, and then extract the output into the probability list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompt loop\n",
    "prompts = []\n",
    "\n",
    "for each_row in df.index:\n",
    "    rule = df[\"rule\"][each_row]\n",
    "    body = df[\"body\"][each_row]\n",
    "    input = \"Calculate the probability that a sample of text violates a rule where, 0 = rule is not violated to 1 = rule is violated. Here is the rule: \" + rule + \". Here is the text: \" + body + \". Structure your response as only a numeric probability ranging from 0.0 to 1.0.\"\n",
    "    prompts.append(input)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of the prompting system but with a single case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tokens = 10 # n characters in output\n",
    "\n",
    "# generate responses\n",
    "responses = []\n",
    "for each_prompt in tqdm(prompts):\n",
    "    input = [{\"role\": \"user\", \"content\": each_prompt}]\n",
    "    output = pipeline(input, max_new_tokens = out_tokens)\n",
    "    new_response = output[0][\"generated_text\"][-1][\"content\"]\n",
    "    responses.append(new_response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Calculate the probability that a sample of text violates a rule where, 0 = rule is not violated to 1 = rule is violated. Here is the rule: \" + df[\"rule\"][0] + \". Here is the text: \" + df[\"body\"][0] + \". Structure your response as only a numeric probability ranging from 0.0 to 1.0.\"\n",
    "input = [{\"role\": \"user\", \"content\": prompt}]\n",
    "output = pipeline(input, max_new_tokens = out_tokens)\n",
    "response = output[0][\"generated_text\"][-1][\"content\"]\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Responses\n",
    "Evaluate the responses based on the area under the curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df[\"rule_violation\"]\n",
    "y_obs = probability\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_obs, pos_label=2)\n",
    "metrics.auc(fpr, tpr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
