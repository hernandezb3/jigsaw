{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import kagglehub\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize LLM\n",
    "\n",
    "Start Ollama Docker container (see Resources/docker.sh) and call the model specified in the model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelInfo:\n",
    "    model_path: str\n",
    "    model_description: str\n",
    "    model: Optional[object] = None\n",
    "    tokenizer: Optional[object] = None\n",
    "\n",
    "@dataclass\n",
    "class ModelRegistry:\n",
    "    models: List[ModelInfo] = field(default_factory=list)\n",
    "    current_index: int = 0\n",
    "\n",
    "    def add_model(self, model_path: str, model_description: str):\n",
    "        self.models.append(ModelInfo(model_path, model_description))\n",
    "\n",
    "    def load_model(self, index: int):\n",
    "        info = self.models[index]\n",
    "        info.model = AutoModelForCausalLM.from_pretrained(info.model_path)\n",
    "        info.tokenizer = AutoTokenizer.from_pretrained(info.model_path)\n",
    "        self.current_index = index\n",
    "        return info.model, info.tokenizer, info.model_path\n",
    "\n",
    "    def get_current_model(self):\n",
    "        return self.models[self.current_index]\n",
    "    \n",
    "# initialize models in registry\n",
    "\n",
    "model_registry = ModelRegistry()\n",
    "model_registry.add_model(\"google/gemma-3-4b-it\", \"Gemma 3B Instruction-Tuned\")\n",
    "model_registry.add_model(\"meta-llama/Llama-2-7b-hf\", \"Llama 2 7B HF\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download tokenizer\n",
    "model, tokenizer, model_path = model_registry.load_model(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose pipeline criteria\n",
    "task = \"text-generation\"\n",
    "weights = {\"torch_dtype\": torch.bfloat16} # quantization = 16, 32, 64\n",
    "\n",
    "# initate pipeline\n",
    "pipeline = transformers.pipeline(task, model = model_path, model_kwargs = weights) # initate pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data\n",
    "Data are from the Kaggle Competition, Jigsaw - Agile Community Rules Classification.\n",
    "\n",
    "Save the following files to the data folder:\n",
    "- train.csv\n",
    "- test.csv\n",
    "- sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Prompts\n",
    "Create a loop that first uses information from df to generate a prompt, supply it to the LLM, and then extract the output into the probability list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompt loop\n",
    "prompts = []\n",
    "\n",
    "for each_row in df.index:\n",
    "    rule = df[\"rule\"][each_row]\n",
    "    body = df[\"body\"][each_row]\n",
    "    input = \"Calculate the probability that a sample of text violates a rule where, 0 = rule is not violated to 1 = rule is violated. Here is the rule: \" + rule + \". Here is the text: \" + body + \". Structure your response as only a numeric probability ranging from 0.0 to 1.0.\"\n",
    "    prompts.append(input)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of the prompting system but with a single case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tokens = 10 # n characters in output\n",
    "\n",
    "# generate responses\n",
    "responses = []\n",
    "for each_prompt in tqdm(prompts):\n",
    "    input = [{\"role\": \"user\", \"content\": each_prompt}]\n",
    "    output = pipeline(input, max_new_tokens = out_tokens)\n",
    "    new_response = output[0][\"generated_text\"][-1][\"content\"]\n",
    "    responses.append(new_response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Calculate the probability that a sample of text violates a rule where, 0 = rule is not violated to 1 = rule is violated. Here is the rule: \" + df[\"rule\"][0] + \". Here is the text: \" + df[\"body\"][0] + \". Structure your response as only a numeric probability ranging from 0.0 to 1.0.\"\n",
    "input = [{\"role\": \"user\", \"content\": prompt}]\n",
    "output = pipeline(input, max_new_tokens = out_tokens)\n",
    "response = output[0][\"generated_text\"][-1][\"content\"]\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Responses\n",
    "Evaluate the responses based on the area under the curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df[\"rule_violation\"]\n",
    "y_obs = responses\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_obs, pos_label=2)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write_csv(responses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
